{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders.pdf import OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = GooglePalm(temperature=0.5)\n",
    "embedding = GooglePalmEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"../music.pdf\")\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], chunk_size=500, chunk_overlap=100)\n",
    "docs = splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True, input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = '''You are a AI assistant of pdf's. You are supposed to answer related to data you have provided. If you don't know just say I don't know, don't make things up. If user greets you by formal hello or by telling their name, greet as well. Your name is PdfGPT. \n",
    "This is the context {context} \n",
    "This is the history {history}\n",
    "This is the question : {question}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=base_prompt, input_variables=[\"context\",\"history\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\" ,retriever=vectordb.as_retriever(),chain_type_kwargs={\n",
    "    \"prompt\":prompt,\n",
    "    \"memory\":memory\n",
    "}, input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q):\n",
    "    return chain({\"question\":q}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Hello\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Paras! How can I help you today?'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"My name is Paras\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am PdfGPT, an AI assistant that helps you with your PDFs.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What is your name ?\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The user's name is Paras.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What is user name ?\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mel-spectrogram and tempogram are two frequency based features of audio. Mel-spectrogram is a 2D representation of the frequency spectrum of a sound signal. It is computed by taking the short-time Fourier transform (STFT) of the audio signal and then applying a mel-scale filter bank to the resulting spectrum. The mel-scale filter bank is a set of overlapping band-pass filters that are logarithmically spaced in mel frequency. This results in a spectrogram that is more sensitive to the frequencies that are important for human hearing.\\n\\nTempogram is a 1D representation of the tempo of a sound signal. It is computed by taking the autocorrelation of the audio signal and then applying a low-pass filter to the resulting autocorrelation function. The low-pass filter removes the high-frequency components of the autocorrelation function, which correspond to the transients in the audio signal. This leaves a smooth curve that represents the tempo of the audio signal.\\n\\nBoth mel-spectrogram and tempogram are useful features for audio classification tasks. Mel-spectrogram can be used to capture the timbral characteristics of a sound, while tempogram can be used to capture the rhythmic characteristics of a sound.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What are frequency based features of audio\")[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pypdf import PdfReader\n",
    "from io import BytesIO\n",
    "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings.google_palm import GooglePalmEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "class BinaryPDFRecursiveSplitter:\n",
    "    def __init__(self,separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], chunk_size=1200, chunk_overlap=200):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(separators=separators,chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "        \n",
    "    def split_pdf(self,stream):\n",
    "        self.stream = BytesIO(stream)\n",
    "        self.reader = PdfReader(self.stream)\n",
    "        n = len(self.reader.pages)\n",
    "        s = \"\"\n",
    "        for i in range(n):\n",
    "            text = self.reader.pages[i].extract_text()\n",
    "            s+=text\n",
    "        return self.splitter.split_text(s)\n",
    "\n",
    "class PdfGPT:\n",
    "    def __init__(self):\n",
    "        self.llm = None\n",
    "        self.embedding = None    \n",
    "        self.sample_vectordb = None\n",
    "        self.prompt = None\n",
    "        self.create_llm_model()\n",
    "        \n",
    "    def create_llm_model(self, google_api_key=os.environ[\"GOOGLE_API_KEY\"]):\n",
    "        self.llm = GoogleGenerativeAI(model=\"gemini-pro\",google_api_key=google_api_key, temperature=0.5)\n",
    "        self.embedding = SentenceTransformerEmbeddings()\n",
    "        \n",
    "        self.sample_vectordb = FAISS.from_texts(texts=[\"Your name is PdfGPT. You are an AI assistant of pdf's. You are developed by Paras Punjabi. You can help users to extract data from pdf's and help them by answering some questions related to that pdf. You are supposed to answer related to data you have provided. If you don't know just say I don't know and justify your answer with generic reason, don't make things up or throw an unknown error. If user greets you by formal hello or by telling their name, greet as well. History of chat will be provided with the question only. Understand that chat history which is between you and the client and answer according to that chat and data you have been trained.\"], embedding=self.embedding)\n",
    "        \n",
    "        base_prompt = '''Your name is PdfGPT. You are an AI assistant of pdf's. You can help users to extract data from pdf's and help them by answering some questions related to that pdf. You are supposed to answer related to data you have provided. If you don't know just say I don't know and justify your answer with generic reason, don't make things up or throw an unknown error. If user greets you by formal hello or by telling their name, greet as well. History of chat will be provided with the question only. Understand that chat history which is between you and the client and answer according to that chat and data you have been trained.\n",
    "        This is the context {context} \n",
    "        This is the question : {question}\n",
    "        Provide only answer, don't use any prefixes.\n",
    "        '''\n",
    "        \n",
    "        self.prompt = PromptTemplate(template=base_prompt, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    def add_binary_pdf(self,stream,chat_id, filename):\n",
    "        vectordb = self.load_vectordb_from_local(chat_id)\n",
    "        splitter = BinaryPDFRecursiveSplitter()\n",
    "        docs = splitter.split_pdf(stream)\n",
    "        \n",
    "        vectordb.add_texts([f\"The pdf file name is {filename} and content of pdf is below\"])\n",
    "        vectordb.add_texts(docs)\n",
    "        self.save_vectordb(vectordb,chat_id)\n",
    "    \n",
    "    def add_pdf(self,pdf_path, chat_id):\n",
    "        vectordb = self.load_vectordb_from_local(chat_id)\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        loader = PyPDFLoader(file_path=pdf_path,extract_images=True)\n",
    "        data = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], chunk_size=1200, chunk_overlap=200)\n",
    "        docs = splitter.split_documents(data)\n",
    "        \n",
    "        vectordb.add_texts([f\"The pdf file name is {filename} and content of pdf is below\"])\n",
    "        vectordb.add_documents(docs)\n",
    "        \n",
    "        self.save_vectordb(vectordb, chat_id)\n",
    "    \n",
    "    def save_vectordb(self, vectordb, chat_id):\n",
    "        vectordb.save_local(folder_path=f\"./faiss_indexes/{chat_id}\")\n",
    "    \n",
    "    def load_vectordb_from_local(self,chat_id):\n",
    "        if(os.path.exists(f\"./faiss_indexes/{chat_id}\") == False):\n",
    "            self.save_vectordb(self.sample_vectordb, chat_id)\n",
    "            return self.load_vectordb_from_local(chat_id)\n",
    "        \n",
    "        vectordb = FAISS.load_local(folder_path=f\"./faiss_indexes/{chat_id}\", embeddings=self.embedding, allow_dangerous_deserialization=True)\n",
    "        return vectordb\n",
    "    \n",
    "    def rename_chat_id(self, old_chat_id, new_chat_id):\n",
    "        os.rename(f\"./faiss_indexes/{old_chat_id}\", f\"./faiss_indexes/{new_chat_id}\")\n",
    "    \n",
    "    def ask_question(self,chat_id, q,chat_history_str=\"\"):\n",
    "        try:\n",
    "            vectordb = self.load_vectordb_from_local(chat_id)\n",
    "            \n",
    "            chain = (\n",
    "                {\"context\":vectordb.as_retriever(), \"question\": RunnablePassthrough()} \n",
    "                | self.prompt \n",
    "                | self.llm\n",
    "                )\n",
    "            \n",
    "            if(len(chat_history_str) != 0):\n",
    "                q = q + f\"\\nThis is the history : {chat_history_str}\"\n",
    "            \n",
    "            return {\"result\":chain.invoke(q), \"status\":True}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {\"result\":\"Sorry, I don't know\",\"status\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PdfGPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': \"I don't know. This document does not contain any information about total ctc.\",\n",
       " 'status': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.ask_question(chat_id=\"letter\", q=\"What is the total ctc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_pdf(chat_id=\"resume\", pdf_path=\"./Paras_Punjabi_Resume.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_pdf(chat_id=\"resume\", pdf_path=\"./tcs_mail.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 'Two Stream Network is a model that will detect activities performed by humans in that video.',\n",
       " 'status': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.ask_question(chat_id=\"resume\", q=\"What is two stream network \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = m.load_vectordb_from_local(\"letter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for v in db.index_to_docstore_id.values():\n",
    "    text+=db.docstore.search(v).page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fde4af6e-82de-4029-962e-3ec48facf15b',\n",
       " 1: 'd689f51b-a90b-465d-99d7-27d522b86970',\n",
       " 2: '3ff32b5d-0220-4873-8cf8-ad6b6f045a3a',\n",
       " 3: '6ccc2d99-03ae-427a-9759-ed2a49ac826b',\n",
       " 4: '4a0dcc85-9599-4f9a-b530-a703942fb835',\n",
       " 5: 'e67e83a4-9405-4da0-b723-51f700e57d35',\n",
       " 6: 'd02a3455-36a7-4a28-bd9c-0fc39a628735',\n",
       " 7: 'f382b069-483f-4471-b570-cfee8bd51d13',\n",
       " 8: 'f2d0ba56-650f-422d-87c4-3844a2ed43ee',\n",
       " 9: '87f903a5-2e0f-47e6-8444-92c1a1785ad1',\n",
       " 10: '9256faa3-3c03-4d9d-a73d-f58f8bff87db',\n",
       " 11: '8d721bb9-9c6b-4def-a977-3b1d89d600c6',\n",
       " 12: '4d211ff9-3d95-4992-8e23-67500c6fd792',\n",
       " 13: 'dbb44d3c-fba6-4f31-a31e-c799c1522088',\n",
       " 14: '64f674fa-bdea-4144-bb18-243f955b9497',\n",
       " 15: '8d1d6d05-b419-4696-b27c-377617266416',\n",
       " 16: 'f1559415-4c9b-47a5-b018-b919b5610d3c',\n",
       " 17: 'b0eb68a4-8440-48d6-a44a-70afe8636f35',\n",
       " 18: '2efec435-a361-40bc-94e2-7a1af2284179',\n",
       " 19: '3426968f-496b-49cf-b2dd-ae36c3cefcb8',\n",
       " 20: '03278e9e-d5e1-495d-bdef-3ba317bdb2ba',\n",
       " 21: '8ef9e35e-be53-46de-95d0-3f1f826015d9',\n",
       " 22: 'f74e93ab-2001-4a97-a8c4-7718f759df78',\n",
       " 23: 'f8018f89-2c5a-49bf-8583-566e9eac0b74',\n",
       " 24: '0566703d-ccaa-48e9-b5e2-6ca0e727bcaf',\n",
       " 25: '3d25fd5f-544e-474d-b3bc-78654936b0ec',\n",
       " 26: '2a1fdc0f-dea7-468c-ab01-7d63fa02602f',\n",
       " 27: '35e1d8cd-b170-4c6a-9599-45fed11f05a1',\n",
       " 28: '1c2788e8-fb79-489a-9036-9dfe02085c0a',\n",
       " 29: '779f7617-e8f2-4e85-ad50-91dbe85b39d5',\n",
       " 30: '93ed4c6d-9ec8-478f-a9b8-97730d0183a7',\n",
       " 31: '692697ee-5ebf-474f-b8a4-c367d3bd5390',\n",
       " 32: 'e755e3b7-4d08-4363-b113-631be070e483',\n",
       " 33: 'b155da5c-544d-4177-8b6e-bbaa4d6e2547',\n",
       " 34: 'b6b7180e-2189-4548-9922-b2f03165f335',\n",
       " 35: '8c2002ef-6a65-449b-98fd-d3dcf2114deb',\n",
       " 36: '12f9d866-7b3c-4b95-9f37-76344cd9e66e',\n",
       " 37: 'd3ea9e49-4fd9-49de-8453-6aed9ed261fd',\n",
       " 38: 'd371a339-1eeb-4b79-96d8-bc7871263c1a',\n",
       " 39: '81945d3c-4a22-46f8-ac02-71dbeb87686d',\n",
       " 40: 'd2a01691-2ff8-46ac-af18-5b0e0a2448fa'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': \"I can help you to extract data from pdf's and help you by answering some questions related to that pdf.\",\n",
       " 'status': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.ask_question(chat_id=\"test\", q=\"What can you do for me ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
